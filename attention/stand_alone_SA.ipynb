{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Stand Alone Self Attention in Vision Models\n",
    "\n",
    "![Stand Alone SA module](images/sasa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Self Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeSelfAttention(Module):\n",
    "    def __init__(self, d_in, d_out, ks, groups, stride=1):\n",
    "        self.n_c, self.ks, self.groups, self.stride = d_out, ks, groups, stride\n",
    "        # linear transformation for queries, values and keys\n",
    "        self.qx, self.kx, self.vx = [ConvLayer(d_in, d_out, ks=1, norm_type=None,\n",
    "                                               act_cls=None) for _ in range(3)]\n",
    "        # positional embeddings\n",
    "        self.row_embeddings = nn.Parameter(torch.randn(d_out//2, ks))\n",
    "        self.col_embeddings = nn.Parameter(torch.randn(d_out//2, ks))\n",
    "        \n",
    "    def calc_out_shape(self, inp_shape, pad):\n",
    "        out_shape = [(sz + 2*pad - self.ks) // self.stride + 1 for sz in inp_shape]\n",
    "        return out_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query, keys, values = self.qx(x), self.kx(x), self.vx(x)\n",
    "        \n",
    "        pad = (self.ks -1) // 2\n",
    "        \n",
    "        # use unfold to extract the memory blocks and their associated queries\n",
    "        query = F.unfold(query, kernel_size=1, stride=self.stride)\n",
    "        keys = F.unfold(keys, kernel_size=self.ks, padding=pad, stride=self.stride)\n",
    "        values = F.unfold(values, kernel_size=self.ks, padding=pad, stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # reshape and permute the dimensions into the appropriate format for matrix multiplication\n",
    "        query = query.view(query.shape[0], self.groups, self.n_c//self.groups, -1, query.shape[-1]) # bs*G*C//G*1*N\n",
    "        query = query.permute(0, 4, 1, 2, 3) # bs * N * G * C//G * 1\n",
    "        keys = keys.view(keys.shape[0], self.groups, self.n_c//self.groups, -1, keys.shape[-1]) # bs*G*C//G*ks^2*N\n",
    "        keys = keys.permute(0, 4, 1, 2, 3) # bs * N * G * C//G * ks^2\n",
    "        values = values.view(values.shape[0], self.groups, self.n_c//self.groups, -1, values.shape[-1]) # bs*G*C//G*ks^2*N\n",
    "        values = values.permute(0, 4, 1, 2, 3) # bs * N * G * C//G * ks^2\n",
    "        \n",
    "        # get positional embeddings\n",
    "        row_embeddings = self.row_embeddings.unsqueeze(-1).expand(-1, -1, self.ks)\n",
    "        col_embeddings = self.col_embeddings.unsqueeze(-2).expand(-1, self.ks, -1)\n",
    "        \n",
    "        embeddings = torch.cat((row_embeddings, col_embeddings)).view(self.groups,\n",
    "                                self.n_c//self.groups, -1) # G * C//G * ks^2\n",
    "        # add empty dimensions to match the shape of keys\n",
    "        embeddings = embeddings[None, None, -1] # 1 * 1 * G * C//G * ks^2\n",
    "        \n",
    "        # compute attention map\n",
    "        att_map = F.softmax(torch.matmul(query.transpose(-2,-1), keys+embeddings), dim=-1)\n",
    "        # compute final output\n",
    "        out = torch.matmul(att_map, values.transpose(-2,-1)).permute(0, 2, 3, 4, 1)\n",
    "        \n",
    "        return out.view(out.shape[0], self.n_c, *self.calc_out_shape(x.shape[-2:], pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(32, 64, 56, 56)\n",
    "inp = inp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = RelativeSelfAttention(64, 128, 7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = sa.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sa(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 56, 56])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelativeSelfAttention (Input shape: ['32 x 64 x 56 x 56'])\n",
       "================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "================================================================\n",
       "Conv2d               32 x 128 x 56 x 56   8,320      True      \n",
       "________________________________________________________________\n",
       "Conv2d               32 x 128 x 56 x 56   8,320      True      \n",
       "________________________________________________________________\n",
       "Conv2d               32 x 128 x 56 x 56   8,320      True      \n",
       "________________________________________________________________\n",
       "\n",
       "Total params: 24,960\n",
       "Total trainable params: 24,960\n",
       "Total non-trainable params: 0\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.summary(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
