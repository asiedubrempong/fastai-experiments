{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a neural network from scratch\n",
    "---\n",
    "Well not completely from scratch, we only make use of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know no machine learning can be done without data, for our purposes, we are going to use simple dataset of test scores for General Psychology, which you can find [here.](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html)\n",
    "The independent variables in this dataset are the scores of a student of three different tests which we would use to predict the score of the student on the final examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('exams.csv', delimiter=',', skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  80.,  75., 152.],\n",
       "       [ 93.,  88.,  93., 185.],\n",
       "       [ 89.,  91.,  90., 180.],\n",
       "       [ 96.,  98., 100., 196.],\n",
       "       [ 73.,  66.,  70., 142.],\n",
       "       [ 53.,  46.,  55., 101.],\n",
       "       [ 69.,  74.,  77., 149.],\n",
       "       [ 47.,  56.,  60., 115.],\n",
       "       [ 87.,  79.,  90., 175.],\n",
       "       [ 79.,  70.,  88., 164.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data contains test scores for 25 students\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the data by shifting the range of the test scores from `0-100` to `0-1` and the range of the final exam scores from `0-200` to `0-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 3), (25,))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = data[:,:3]/100\n",
    "y = data[:, 3]/200\n",
    "x_norm.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network which we will be building in this work is 2-layer fully connected neural network with 5 neurons in the hidden layer and one output neuron. There are 3 inputs corresponding to the three tests.\n",
    "![network architecture](nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks pass data forward through a \"network\", where each layer performs an affine (linear) transformation of it's inputs. The result of this affine transformation is then passed through an activation function before being fed to the next layer. Mathematically, this is given by $$y=f(W^Tx + b)$$\n",
    "The activation function that would be used here is the rectified linear unit (ReLU) ie. $$f(x) = max(0, x)$$ \n",
    "With this our earlier equation can be rewritten as: $$y=max(0, W^Tx + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function each for the linear transformation and the activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b):\n",
    "    return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(inp):\n",
    "    return inp.clip(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try them out on a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1, 3) # our inputs\n",
    "w = np.random.randn(3,5) # weights\n",
    "b = np.zeros(5) # bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.66940703, 0.3047442 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = relu(lin(x, w, b))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then to be able to train a neural network, we need a loss function which would give us a measure of how the model is performing and which would also be used as a signal to update the parameters of the network. Because this is a regression problem, we are going to use the mean square error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, targ):\n",
    "    return np.square(pred.squeeze() - targ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go on and define our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was said earlier, we are building a 2-layer neural network, hence we would a weight matrix as well as a bias vector for each layer of the neural network. Also weight initialization is very crucial to training of any neural network. Because we are using ReLU as our activation function, the weights in our network are going to be initialized using the Kaiming initialization scheme. With this scheme, we scale the weights by $\\sqrt{2 / n_{in}}$, where $n_{in}$ is the number of inputs of our model. For a more thorough treatment of weight initialization schemes, refer to chapter 17 of the [fastai book](https://github.com/fastai/fastbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816496580927726"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.randn(3,5) / np.sqrt(2/3) # we have 3 inputs to the first layer\n",
    "b1 = np.zeros(5)\n",
    "w2 = np.random.randn(5,1) / np.sqrt(2/5) # we have 5 inputs to the second layer\n",
    "b2 = np.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    l1 = lin(x, w1, b1) # first layer\n",
    "    l2 = relu(l1) # activation function\n",
    "    l3 = lin(l2, w2, b2) # second layer\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's perform a single forward pass with our data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.24149793, 3.58553564, 3.69885022, 3.95548618, 2.71748319,\n",
       "       1.85480527, 2.93960527, 2.15877107, 3.18991937, 2.76586399,\n",
       "       2.8144108 , 2.61014599, 3.89046004, 3.31727815, 2.89868341,\n",
       "       3.59776002, 3.15133406, 3.55379515, 3.76838344, 3.39218589,\n",
       "       3.42851455, 3.31251271, 3.31588894, 3.43050661, 3.79899309])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(x_norm)\n",
    "y_pred.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our loss is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.952290560077333"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gotten to the most important part of the whole notebook. With the help of the chain rule, we have to find the gradients of the loss with respect to each of our model parameters. We then take a step in the negative direction of the gradient, because the gradient points in the direction of `steepest ascent`, taking a step in the negative direction would mean we move in the direction of `steepest descent` ie. the fastest direction that would take us to minimum of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we would define backward functions for each of the functions involved in the forward pass. This function would calculate the gradients of the output that particular function with respect to it's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(pred, targ):\n",
    "    pred.grad = (2 * (pred.squeeze() - targ) / pred.shape[0])[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of ReLU is 0 for values less than or equal to zero and 1 for all other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, output):\n",
    "    inp.grad = output.grad * (inp > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(x, w, b, output):\n",
    "    x.grad = output.grad @ w.transpose()\n",
    "    w.grad = x.transpose() @ output.grad\n",
    "    b.grad = output.grad.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call these functions in a reverse fashion as to how they were used in the forward pass, since each intermediate function would need the gradients of the output to be able to find the gradient of it's inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(x, y, w1, b1, w2, b2):\n",
    "    # forward pass\n",
    "    l1 = lin(x, w1, b1) # first layer\n",
    "    l2 = relu(l1) # activation function\n",
    "    l3 = lin(l2, w2, b2) # second layer\n",
    "    \n",
    "    loss = mse(l3, y)\n",
    "    \n",
    "    # backward pass\n",
    "    mse_grad(l3, y)\n",
    "    lin_grad(l2, w2, b2, l3)\n",
    "    relu_grad(l1, l2)\n",
    "    lin_grad(x, w1, b1, l1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-7e0aadde4a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_and_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-0e475b643dc9>\u001b[0m in \u001b[0;36mforward_and_backward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmse_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlin_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrelu_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-90603af86639>\u001b[0m in \u001b[0;36mmse_grad\u001b[0;34m(pred, targ)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmse_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "loss = forward_and_backward(x_norm, y, w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ooooops why???????**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to make use of a `grad` property on the numpy arrays which they don't possess by default. To fix this, we're going to use view casting to return a view of the arrays as another subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.zeros(5)\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now cast this array to be an object of the class `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_arr = arr.view(C)\n",
    "c_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.C"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now add a property to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.54140022, -0.50922137, -0.96793728,  0.42567579, -0.78225709])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_arr.grad = np.random.randn(5)\n",
    "c_arr.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With this, we have to redefine our arrays and cast them to be objects of type `C`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = (data[:,:3]/100).view(C)\n",
    "y = (data[:, 3]/200).view(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = (np.random.randn(3,5) / np.sqrt(2/3)).view(C) # we have 3 inputs to the first layer\n",
    "b1 = (np.zeros(5)).view(C)\n",
    "w2 = (np.random.randn(5,1) / np.sqrt(2/5)).view(C) # we have 5 inputs to the second layer\n",
    "b2 = (np.zeros(1)).view(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C(11.23153738)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = forward_and_backward(x_norm, y, w1, b1, w2, b2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C([[3.68849716, 0.        , 5.28711076, 0.        , 0.        ],\n",
       "   [3.70495381, 0.        , 5.3106998 , 0.        , 0.        ],\n",
       "   [3.79474304, 0.        , 5.43940415, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to update our parameters, so that the model can improve it's performance. We do this using a learning rate which controls how much of a step we take in the direction of the negative of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(x, y, w1, b1, w2, b2, lr):\n",
    "    # forward pass\n",
    "    l1 = lin(x, w1, b1) # first layer\n",
    "    l2 = relu(l1) # activation function\n",
    "    l3 = lin(l2, w2, b2) # second layer\n",
    "    \n",
    "    loss = mse(l3, y)\n",
    "    \n",
    "    # backward pass\n",
    "    mse_grad(l3, y)\n",
    "    lin_grad(l2, w2, b2, l3)\n",
    "    relu_grad(l1, l2)\n",
    "    lin_grad(x, w1, b1, l1)\n",
    "    \n",
    "    # update the parameters\n",
    "    w1 -= lr*w1.grad\n",
    "    b1 -= lr*b1.grad\n",
    "    w2 -= lr*w2.grad\n",
    "    b2 -= lr*b2.grad\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train for 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/30 \t\t loss: 36.0464\n",
      "epoch: 2/30 \t\t loss: 11.5179\n",
      "epoch: 3/30 \t\t loss: 6.1279\n",
      "epoch: 4/30 \t\t loss: 3.5722\n",
      "epoch: 5/30 \t\t loss: 2.1787\n",
      "epoch: 6/30 \t\t loss: 1.3616\n",
      "epoch: 7/30 \t\t loss: 0.8627\n",
      "epoch: 8/30 \t\t loss: 0.5510\n",
      "epoch: 9/30 \t\t loss: 0.3537\n",
      "epoch: 10/30 \t\t loss: 0.2280\n",
      "epoch: 11/30 \t\t loss: 0.1475\n",
      "epoch: 12/30 \t\t loss: 0.0959\n",
      "epoch: 13/30 \t\t loss: 0.0629\n",
      "epoch: 14/30 \t\t loss: 0.0417\n",
      "epoch: 15/30 \t\t loss: 0.0281\n",
      "epoch: 16/30 \t\t loss: 0.0194\n",
      "epoch: 17/30 \t\t loss: 0.0139\n",
      "epoch: 18/30 \t\t loss: 0.0103\n",
      "epoch: 19/30 \t\t loss: 0.0081\n",
      "epoch: 20/30 \t\t loss: 0.0066\n",
      "epoch: 21/30 \t\t loss: 0.0057\n",
      "epoch: 22/30 \t\t loss: 0.0051\n",
      "epoch: 23/30 \t\t loss: 0.0047\n",
      "epoch: 24/30 \t\t loss: 0.0045\n",
      "epoch: 25/30 \t\t loss: 0.0043\n",
      "epoch: 26/30 \t\t loss: 0.0042\n",
      "epoch: 27/30 \t\t loss: 0.0041\n",
      "epoch: 28/30 \t\t loss: 0.0041\n",
      "epoch: 29/30 \t\t loss: 0.0041\n",
      "epoch: 30/30 \t\t loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    loss = forward_and_backward(x_norm, y, w1, b1, w2, b2, lr)\n",
    "    print(f'epoch: {i+1}/30 \\t\\t loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we've got every part working now. Let's focus on refactoring the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can combine the forward and backward functions into classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, pred, targ):\n",
    "        self.pred = pred\n",
    "        self.targ = targ\n",
    "        return np.square(pred.squeeze() - targ).mean()\n",
    "    \n",
    "    def backward(self):\n",
    "        self.pred.grad = (2 * (self.pred.squeeze() - self.targ) / self.pred.shape[0])[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.output = inp.clip(0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.grad = self.output.grad * (self.inp > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.inp = x\n",
    "        self.output = x@self.w + self.b\n",
    "        print(self.inp.shape)\n",
    "        print(self.output.shape)\n",
    "        print('************')\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self):\n",
    "        print(self.inp.shape)\n",
    "        print(self.output.shape)\n",
    "        print(self.w.shape)\n",
    "        self.inp.grad = self.output.grad @ self.w.transpose()\n",
    "        self.w.grad = self.inp.transpose() @ self.output.grad\n",
    "        self.b.grad = self.output.grad.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2, lr):\n",
    "        self.w1, self.w2 = w1, w2\n",
    "        self.b1, self.b2 = b1, b2\n",
    "        self.lr = lr\n",
    "        self.loss = Mse()\n",
    "        self.layers = [Lin(w1, b1), ReLU(), Lin(w2, b2)]\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return self.forward(x, y)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, y)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()\n",
    "        # update the parameters\n",
    "        self.w1 -= lr*self.w1.grad\n",
    "        self.b1 -= lr*self.b1.grad\n",
    "        self.w2 -= lr*self.w2.grad\n",
    "        self.b2 -= lr*self.b2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the forward and backward passes are now easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = (data[:,:3]/100).view(C)\n",
    "y = (data[:, 3]/200).view(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 3), (25,))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = (np.random.randn(3,5) / np.sqrt(2/3)).view(C) # we have 3 inputs to the first layer\n",
    "b1 = (np.zeros(5)).view(C)\n",
    "w2 = (np.random.randn(5,1) / np.sqrt(2/5)).view(C) # we have 5 inputs to the second layer\n",
    "b2 = (np.zeros(1)).view(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C(0.07339913)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(x, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C([[-0.46032947,  0.38936315,  0.83624969, -1.93938153, -2.50546059],\n",
       "   [ 1.02329089,  0.2531961 ,  2.54961788, -2.13072417,  1.19059811],\n",
       "   [ 1.5174681 ,  1.12563573, -1.42631003, -0.09963745,  0.65060765]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "(1, 1)\n",
      "(5, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 25 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-f1de3ca3ec99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-181-10e8d4a28f80>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-a8242fb885d7>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 25 is different from 1)"
     ]
    }
   ],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
